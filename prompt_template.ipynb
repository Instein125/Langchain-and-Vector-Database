{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the question based on the context given below. If the question can not be answered with the provided context then answer \n",
    "with I dont know. \n",
    "\n",
    "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
    "\n",
    "question: {question}\n",
    "answer: \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template= template, input_variables=['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(prompt= prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data1 = {\"question\": \"What is quantum computing?\"}\n",
    "input_data2 = {\"question\": \"What is World highest moutain?\"}\n",
    "\n",
    "response1 = chain.run(input_data1)\n",
    "response2 = chain.run(input_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n"
     ]
    }
   ],
   "source": [
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context does not contain any information about the world's highest mountain.\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Scarcastic Ai Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        'answer': \"Try talking to a rubber duck; it works wonders.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    template= example_template,\n",
    "    input_variables = ['query', 'answer']\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative and funny responses to users' questions. Here are some\n",
    "examples:\"\"\"\n",
    "\n",
    "suffix = \"\"\"User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=['query'],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
    "response = chain.run(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By reading tea leaves and staring at a crystal ball.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Human/AI messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0, convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates the english language to pirate language\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template=template)\n",
    "\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "\n",
    "human_message = '{text}'\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,\n",
    "                                                example_human,\n",
    "                                                example_ai,\n",
    "                                                human_message_prompt\n",
    "])\n",
    "\n",
    "chain = LLMChain(llm=chat_llm, prompt=chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopCandidateException",
     "evalue": "index: 0\ncontent {\n  parts {\n    text: \"Ahoy there, ye landlubber! Ye be speakin\\' of yer love fer dogs, be ye? Well, let me tell ye, there be nothin\\' finer than a loyal pooch by yer side. Dogs be man\\'s best matey, and they\\'ll be there fer ye through thick and thin. They\\'ll wag their tails and lick yer face, and they\\'ll never ask fer nothin\\' in return. So if ye be lookin\\' fer a true friend, get yerself a dog, me hearty!\"\n  }\n  role: \"model\"\n}\nfinish_reason: SAFETY\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: LOW\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: HIGH\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: NEGLIGIBLE\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopCandidateException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\prompt_template.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dell/OneDrive/Documents/LangChain%20and%20Vector%20database/prompt_template.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mI love dogs.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:507\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    506\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[0;32m    508\u001b[0m         _output_key\n\u001b[0;32m    509\u001b[0m     ]\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[0;32m    513\u001b[0m         _output_key\n\u001b[0;32m    514\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    116\u001b[0m         prompts,\n\u001b[0;32m    117\u001b[0m         stop,\n\u001b[0;32m    118\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    119\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs,\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mbind(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs)\u001b[39m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:496\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    490\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    495\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(prompt_messages, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:383\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[0;32m    382\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[1;32m--> 383\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    384\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    385\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[0;32m    386\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[0;32m    387\u001b[0m ]\n\u001b[0;32m    388\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:373\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[0;32m    371\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    372\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 373\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    374\u001b[0m                 m,\n\u001b[0;32m    375\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    376\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[i] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    377\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    378\u001b[0m             )\n\u001b[0;32m    379\u001b[0m         )\n\u001b[0;32m    380\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    381\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:529\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    526\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 529\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    530\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain_google_genai\\chat_models.py:550\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate\u001b[39m(\n\u001b[0;32m    543\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    544\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    548\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResult:\n\u001b[0;32m    549\u001b[0m     params, chat, message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(messages, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m--> 550\u001b[0m     response: genai\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mGenerateContentResponse \u001b[39m=\u001b[39m _chat_with_retry(\n\u001b[0;32m    551\u001b[0m         content\u001b[39m=\u001b[39mmessage,\n\u001b[0;32m    552\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    553\u001b[0m         generation_method\u001b[39m=\u001b[39mchat\u001b[39m.\u001b[39msend_message,\n\u001b[0;32m    554\u001b[0m     )\n\u001b[0;32m    555\u001b[0m     \u001b[39mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain_google_genai\\chat_models.py:140\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    138\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m _chat_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain_google_genai\\chat_models.py:138\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[39mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    135\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid argument provided to Gemini: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\langchain_google_genai\\chat_models.py:131\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chat_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    130\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         \u001b[39mreturn\u001b[39;00m generation_method(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    132\u001b[0m     \u001b[39mexcept\u001b[39;00m InvalidArgument \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    133\u001b[0m         \u001b[39m# Do not retry for these errors.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m         \u001b[39mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    135\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid argument provided to Gemini: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\google\\generativeai\\generative_models.py:384\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[1;34m(self, content, generation_config, safety_settings, stream, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mcandidates[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfinish_reason \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\n\u001b[0;32m    380\u001b[0m         glm\u001b[39m.\u001b[39mCandidate\u001b[39m.\u001b[39mFinishReason\u001b[39m.\u001b[39mFINISH_REASON_UNSPECIFIED,\n\u001b[0;32m    381\u001b[0m         glm\u001b[39m.\u001b[39mCandidate\u001b[39m.\u001b[39mFinishReason\u001b[39m.\u001b[39mSTOP,\n\u001b[0;32m    382\u001b[0m         glm\u001b[39m.\u001b[39mCandidate\u001b[39m.\u001b[39mFinishReason\u001b[39m.\u001b[39mMAX_TOKENS,\n\u001b[0;32m    383\u001b[0m     ):\n\u001b[1;32m--> 384\u001b[0m         \u001b[39mraise\u001b[39;00m generation_types\u001b[39m.\u001b[39mStopCandidateException(response\u001b[39m.\u001b[39mcandidates[\u001b[39m0\u001b[39m])\n\u001b[0;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_sent \u001b[39m=\u001b[39m content\n\u001b[0;32m    387\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_received \u001b[39m=\u001b[39m response\n",
      "\u001b[1;31mStopCandidateException\u001b[0m: index: 0\ncontent {\n  parts {\n    text: \"Ahoy there, ye landlubber! Ye be speakin\\' of yer love fer dogs, be ye? Well, let me tell ye, there be nothin\\' finer than a loyal pooch by yer side. Dogs be man\\'s best matey, and they\\'ll be there fer ye through thick and thin. They\\'ll wag their tails and lick yer face, and they\\'ll never ask fer nothin\\' in return. So if ye be lookin\\' fer a true friend, get yerself a dog, me hearty!\"\n  }\n  role: \"model\"\n}\nfinish_reason: SAFETY\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: LOW\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: HIGH\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: NEGLIGIBLE\n}\n"
     ]
    }
   ],
   "source": [
    "chain.run(\"I love dogs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
    "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(template=example_template, input_variables=['word', 'antonym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector = example_selector,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix = \"Give the antonym of every input\",\n",
    "    suffix = 'Word: {input}\\nAntonym:',\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "Word: energetic\n",
      "Antonym: lethargic\n",
      "\n",
      "Word: sunny\n",
      "Antonym: gloomy\n",
      "\n",
      "Word: windy\n",
      "Antonym: calm\n",
      "\n",
      "Word: big\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt.format(input=\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics Similarity Example Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\OneDrive\\Documents\\LangChain and Vector database\\langchain\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.13) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some examples\n",
    "examples = [\n",
    "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
    "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
    "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
    "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
    "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
    "]\n",
    "\n",
    "# Create a PromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "# create Deep Lake dataset\n",
    "my_activeloop_org_id = \"samman\" \n",
    "my_activeloop_dataset_name = \"langchain_course_fewshot_selector\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function\n",
    "embeddings = GooglePalmEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 5 embeddings in 1 batches of size 5:: 100%|██████████| 1/1 [00:04<00:00,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype     shape     dtype  compression\n",
      "  -------    -------   -------   -------  ------- \n",
      "   text       text      (5, 1)     str     None   \n",
      " metadata     json      (5, 1)     str     None   \n",
      " embedding  embedding  (5, 768)  float32   None   \n",
      "    id        text      (5, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SemanticSimilarityExampleSelector using the examples\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(examples=examples, embeddings=embeddings, vectorstore_cls=db, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FewShotPromptTemplate using the example_selector\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
    "    suffix=\"Input: {temperature}\\nOutput:\",\n",
    "    input_variables=['temperature']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 10°C\n",
      "Output: 50°F\n",
      "\n",
      "Input: 10°C\n",
      "Output:\n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 30°C\n",
      "Output: 86°F\n",
      "\n",
      "Input: 30°C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    " #Test the similar_prompt with different inputs\n",
    "print(similar_prompt.format(temperature=\"10°C\"))   # Test with an input\n",
    "print(similar_prompt.format(temperature=\"30°C\"))  # Test with another input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 1 embeddings in 1 batches of size 1:: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype     shape     dtype  compression\n",
      "  -------    -------   -------   -------  ------- \n",
      "   text       text      (6, 1)     str     None   \n",
      " metadata     json      (6, 1)     str     None   \n",
      " embedding  embedding  (6, 768)  float32   None   \n",
      "    id        text      (6, 1)     str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 40°C\n",
      "Output: 104°F\n",
      "\n",
      "Input: 40°C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Add a new example to the SemanticSimilarityExampleSelector\n",
    "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
    "print(similar_prompt.format(temperature=\"40°C\")) # Test with a new input after adding the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
