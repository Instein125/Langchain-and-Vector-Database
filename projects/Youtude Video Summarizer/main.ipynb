{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Workflow:\n",
    "1. Download the YouTube audio file.\n",
    "2. Transcribe the audio using Whisper.\n",
    "3. Summarize the transcribed text using LangChain with three different approaches: stuff, refine, and map_reduce.\n",
    "4. Adding multiple URLs to DeepLake database, and retrieving information. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Packages to be used:\\nyt_dlp\\nwhisper\\nlangchian\\ndeeplake\\nlangchain_generative_ai\\ntiktoken'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Packages to be used:\n",
    "yt_dlp\n",
    "whisper\n",
    "langchian\n",
    "deeplake\n",
    "langchain_generative_ai\n",
    "tiktoken\n",
    "ffmpeg\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to download youtube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "def download_mp4_from_youtube(url, file_name='lecuninterview.mp4'):\n",
    "    #setting options for download\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "        'outtmpl': file_name,\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    #downloading video\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=mBjPyte2ZZo\"\n",
    "download_mp4_from_youtube(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using whisper to transcibe the downloaded video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model('base')\n",
    "result = model.transcribe('lecuninterview.mp4')\n",
    "print(result)\n",
    "\n",
    "#requires strong GPU \n",
    "# for this I am using colab to download the transcribed text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the transcribed text in text file\n",
    "with open('text.txt', 'w') as f:\n",
    "    f.write(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap=50,\n",
    "    separators=[\" \", \",\", \"\\n\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open('text.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text=text)\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Map reduce\n",
    "- \"map-reduce\" and \"refine\" approaches offer more sophisticated ways to process and extract useful information from longer documents.\n",
    "- the \"map-reduce\" method can be parallelized, resulting in faster processing times, the \"refine\" approach is empirically known to produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Jan LeCoon, a pioneer in deep learning, discusses the limitations of large language models and\n",
      "introduces his new joint embedding predictive architecture (JEPA) as a potential solution. - JEPA\n",
      "aims to address the lack of a world model in large language models by learning representations of\n",
      "text that can be used for downstream tasks. - Pre-training transformer architectures involve\n",
      "removing words from a text and training a neural network to predict the missing words, helping the\n",
      "system learn good representations of text. - Large language models can generate text by predicting\n",
      "the next word in a sequence but struggle to represent uncertain predictions, making it difficult to\n",
      "handle scenarios with multiple possible words.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "chain = load_summarize_chain(llm=llm, chain_type='map_reduce')\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary with bullet points using stuff chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"stuff\" approach is the simplest and most naive one, in which all the text from the documents is used in a single prompt.\n",
    "- This method may raise exceptions if all text is longer than the available context size of the LLM and may not be the most efficient way to handle large amounts of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "write a concise summary of the given text in bullet points:\n",
    "{text}\n",
    "\n",
    "Summary in Bullet points:\n",
    "\"\"\"\n",
    "\n",
    "bullet_point_prompt = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables=['text'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Jan LeCoon, a prominent figure in deep learning development, discussed self-supervised learning and its relationship to large language models.\n",
      "- Self-supervised learning has revolutionized natural language processing by pre-training transformer architectures.\n",
      "- Large language models can predict the next word in a text and generate text spontaneously.\n",
      "- Generative models, like large language models, struggle to represent uncertain predictions.\n",
      "- Jan LeCoon introduced his joint embedding predictive architecture (JEPA), which aims to address the limitations of large language models.\n",
      "- JEPA combines self-supervised learning with a world model to improve the representation of uncertainty and enable reasoning about the world.\n",
      "- Jan LeCoon believes that AI systems have the potential to exhibit features of consciousness in the future.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    prompt = bullet_point_prompt\n",
    ")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using refine chian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Craig Smith interviews Jan LeCoon, a pioneer in deep learning and advocate for self-supervised\n",
      "learning. LeCoon discusses the limitations of large language models, particularly their lack of a\n",
      "world model. He introduces his new joint embedding predictive architecture (JEPA) as a potential\n",
      "solution to this problem. LeCoon also shares his theory of consciousness and the possibility of AI\n",
      "systems exhibiting conscious features in the future.  Additionally, LeCoon highlights the\n",
      "significance of self-supervised learning in natural language processing, particularly in pre-\n",
      "training transformer architectures. He emphasizes the transformative impact of self-supervised\n",
      "learning in this field, explaining how it involves training a neural network to predict missing\n",
      "words in a text, resulting in learned representations of text that can be used for various\n",
      "downstream tasks. This approach has revolutionized the field and has practical applications in\n",
      "content moderation systems and other areas.  LeCoon further discusses the limitations of generative\n",
      "models, such as large language models, in representing uncertain predictions. He explains that these\n",
      "models struggle to capture the uncertainty associated with missing information, making it difficult\n",
      "to handle scenarios where multiple options are equally probable.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Transcripts to Deep Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding transcription from multiple videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "def download_mp4_from_youtube(urls, job_id):\n",
    "    # This will hold the titles and authors of each downloaded video\n",
    "    video_info = []\n",
    "\n",
    "    for i, urls in enumerate(urls):\n",
    "        # Set the options for the download\n",
    "        file_name = f'./{job_id}_{i}.mp4'\n",
    "        ydl_opts = {\n",
    "            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "            'outtmpl': file_name,\n",
    "            'quite': True,\n",
    "        }\n",
    "\n",
    "        # Downloading the video \n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            result = ydl.extract_info(url, download=True)\n",
    "            title = result.get('tittle', \"\")\n",
    "            author = result.get('uploader', \"\")\n",
    "\n",
    "        # Adding the title and author to our list\n",
    "            video_info.append((file_name, title, author))\n",
    "\n",
    "    \n",
    "    return video_info\n",
    "\n",
    "\n",
    "urls=[\"https://www.youtube.com/watch?v=mBjPyte2ZZo&t=78s\",\n",
    "    \"https://www.youtube.com/watch?v=cjs7QKJNVYM\",]\n",
    "videos_details = download_mp4_from_youtube(urls, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model('base')\n",
    "\n",
    "results = []\n",
    "\n",
    "#Iterating through each video to transcribe\n",
    "for video in videos_details:\n",
    "    result = whisper.transcribe(video[0])\n",
    "    results.append(result['text'])\n",
    "\n",
    "\n",
    "with open(\"text_multiple.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_multiple.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Hi, I'm Craig Smith and this is I on A On. This week I talk to Jan LeCoon, one of the seminal figures in deep learning development and a long time proponent of self-supervised learning. Jan spoke about what's missing in large language models and about his new joint embedding predictive architecture which may be a step toward filling that gap. He also talked about his theory of consciousness and the potential for AI systems to someday exhibit the features of consciousness. It's a fascinating conversation that I hope you'll enjoy. Okay, so Jan, it's great to see you again. I wanted to talk to you about where you've gone with so supervised learning since last week spoke. In particular, I'm interested in how it relates to large language models because the large language models really came on stream since we spoke. In fact, in your talk about JEPA, which is joint embedding predictive architecture. There you go. Thank you. You mentioned that large language models lack a world model. I\"),\n",
       " Document(page_content=\"that large language models lack a world model. I wanted to talk first about where you've gone with self-supervised learning and where this latest paper stands in your trajectory. But to start, if you could just introduce yourself and we'll go from there. Okay, so my name is Jan Le Ka or Jan Le Koon who want to do it in Gilleswee and I'm a professor at New York University and at the Quarantine Institute in the Center for Data Science. And I'm also the chief AI scientist at Fair, which is the fundamental AI research lab. That's what Fair stands for. Admetta, Neil, Facebook. So tell me about where you've gone with self-supervised learning, how the joint embedding predictive architecture fits into your research. And then if you could talk about how that relates to what's lacking in large language models. Okay, self-supervised learning has been, has basically brought about a revolution in natural language processing because of their use for pre-training transformer architectures. And the\"),\n",
       " Document(page_content=\"pre-training transformer architectures. And the fact that we use transformer architectures for that is somewhat orthogonal to the fact that we use self-supervised learning. But the way those systems are trained is that you take a piece of text, you remove some of the words, you replace them by black markers, and then you train the very large neural net to predict the words that are missing. That's a pre-training phase. And then in the process of training itself to do so, the system learns good representations of text that you can then use as input to its subsequent downstream task, I don't know, translation or Hitchbitch detection or something like that. So that's been a career revolution over the last three or four years. And including in sort of very practical applications, like every sort of type of performing contact moderation systems on Facebook, Google, YouTube, et cetera, use this kind of technique. And there's all kinds of other applications. Now, large language models are\"),\n",
       " Document(page_content=\"applications. Now, large language models are partially this, but also the idea that you can train those things to just predict the next word in a text. And if you use that, you can have those system generate text spontaneously. So there's a few issues with this. First of all, those things are what's called generative models in the sense that they predict the words, the information that is missing, words in this case. And the problem with generative models is that it's very difficult to represent uncertain predictions. So in the case of words, it's easy because we just have the system produce essentially what amounts to a score or a probability for every word in the dictionary. And so it cannot tell you if the word missing in a sentence like the blank chases the mouse in the kitchen. It's probably a cat, could be a dog, but it's probably a cat, right? So you have some distribution of probability over all words in the dictionary. And you can handle uncertainty in the prediction this\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building deeplake store and storing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "my_activeloop_org_id = \"samman\"\n",
    "my_activeloop_dataset_name = \"langchain_course_youtube_summarizer\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 4 embeddings in 1 batches of size 4:: 100%|██████████| 1/1 [00:25<00:00, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://samman/langchain_course_youtube_summarizer', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype     shape     dtype  compression\n",
      "  -------    -------   -------   -------  ------- \n",
      "   text       text      (4, 1)     str     None   \n",
      " metadata     json      (4, 1)     str     None   \n",
      " embedding  embedding  (4, 768)  float32   None   \n",
      "    id        text      (4, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['513d282f-b940-11ee-b60b-60189524c791',\n",
       " '513d2830-b940-11ee-98a7-60189524c791',\n",
       " '513d2831-b940-11ee-b082-60189524c791',\n",
       " '513d2832-b940-11ee-b8f9-60189524c791']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question in bullet points and summarized. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Summarized answer in bullter points:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Large language models are partially generative models, which predict the next word in a text.\n",
      "- Generative models have difficulty representing uncertain predictions.\n",
      "- Large language models lack a world model.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain_type_kwargs = {'prompt': PROMPT}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "print( qa.run(\"Summarize the mentions of large language model\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
