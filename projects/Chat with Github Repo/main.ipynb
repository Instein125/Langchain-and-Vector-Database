{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The aim of this project is to built a chatbot that allows user to chat with github repos.\n",
    "It involves the following process:\n",
    "1. Processing the Repository files\n",
    "2. Saving the embeddings in Deeplake database\n",
    "3. Retrieving from database based on user query\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Repository Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def clone_repo(repo_url, local_path):\n",
    "    \"\"\"Function to clone the repo to given local path\"\"\"\n",
    "    subprocess.run(['git', 'clone', repo_url, local_path])\n",
    "\n",
    "\n",
    "repo_url = \"https://github.com/Instein125/Speech-Denoiser-using-Deep-Learning\"\n",
    "local_path = 'repos/'\n",
    "\n",
    "clone_repo(repo_url, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_docs(root_dir, file_extensions = None):\n",
    "    \"\"\"\n",
    "    Load documents from the specified root directory.\n",
    "    Optionally filter by file extensions.\n",
    "    \"\"\"\n",
    "\n",
    "    docs =[]\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "\n",
    "            # Skip dotfiles\n",
    "            if file.startswith(\".\"):\n",
    "                continue\n",
    "\n",
    "            if file_extensions and os.path.splitext(file)[1] not in file_extensions:\n",
    "                continue\n",
    "\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "\n",
    "    return docs\n",
    "\n",
    "root_dir = \"repos/\"\n",
    "file_extensions=['.md', '.txt', '.py']\n",
    "docs = load_docs(root_dir, file_extensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='import streamlit as st\\nimport numpy as np\\nfrom scipy.io.wavfile import write\\nimport librosa\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nfrom unet import UNET\\nimport soundfile as sf\\n\\n\\n# \"\"\"\\n# 1. Select noised audio file\\n# 2. Load noised audio file\\n# 3. Extract spectrogram\\n# 4. Normalize and denormalize\\n# 4. Change the shape of spectrogram\\n# 5. Create a unet model with desired shape\\n# 6. Load model weights\\n# 7. Contruct the denoised audio\\n# 8. Plot the waveform, spectrogram\\n# \"\"\"\\n\\ndef load_audio(y):\\n    return librosa.load(y)\\n\\ndef extract(signal, frame_size=512, hop_length=256):\\n        stft = librosa.stft(signal,\\n                                n_fft=frame_size,\\n                                hop_length=hop_length)\\n        spectrogram = np.abs(stft)\\n        log_spectrogram = librosa.amplitude_to_db(spectrogram)\\n        return log_spectrogram\\n\\ndef normalize(spectrogram, min_val, max_val):\\n    normalized_spectrogram = (spectrogram - min_val) / (max_val - min_val)\\n    return normalized_spectrogram\\n\\ndef denormalize(spectrogram, original_min, original_max):\\n    denormalized_spectrogram = spectrogram * (original_max - original_min) + original_min\\n    return denormalized_spectrogram\\n\\ndef load_numpy(spectrogram, final_shape):\\n    arr = []\\n    if spectrogram.shape[1] < final_shape[1]:\\n    # Pad the spectrogram to match the desired final shape\\n        pad_width = ((0, 0), (0, final_shape[1] - spectrogram.shape[1]))\\n        processed_spectrogram = np.pad(spectrogram, pad_width, mode=\\'constant\\', constant_values=0)\\n    elif spectrogram.shape[1] > final_shape[1]:\\n     # Trim the spectrogram to match the desired final shape\\n        processed_spectrogram = spectrogram[:final_shape[0], :final_shape[1]]\\n    else:\\n        processed_spectrogram = spectrogram  # No change needed if the shape is already as desired\\n    \\n    # Append the padded spectrogram to the x_train list\\n    processed_spectrogram = processed_spectrogram[:final_shape[0], :]\\n    arr.append(processed_spectrogram)\\n    # Convert the list to a NumPy array if required\\n    arr = np.array(arr)\\n    # reshaping to make it suitable for training\\n    arr = arr.reshape(arr.shape[0], arr.shape[1], arr.shape[2], 1 )\\n    # print(arr.shape)\\n    return arr\\n\\ndef create_model(final_shape):\\n    model = UNET(\\n        input_shape=(final_shape[0], final_shape[1], 1),\\n        conv_filters=(64, 128, 256, 512),\\n        conv_kernels=(3, 3),\\n    )\\n    model.load_weights(\\'best_weight.hdf5\\')\\n    return model\\n\\ndef convert_spectrogram_to_audio( spectrogram, min_val, max_val):\\n        \\n        # reshape the log spectrogram\\n        spectrogram = spectrogram.reshape(spectrogram.shape[0], spectrogram.shape[1],)\\n        # apply denormalisation\\n        denorm_log_spec = denormalize(\\n            spectrogram, min_val, max_val)\\n        # log spectrogram -> spectrogram\\n        spec = librosa.db_to_amplitude(denorm_log_spec)\\n        # apply Griffin-Lim\\n        signal = librosa.istft(spec, hop_length=256)\\n        return signal, spectrogram\\n        return signal, spectrogram\\n\\ndef plot_waveform(y, title=\"Waveform\"):\\n    # Generate time values for x-axis\\n\\n    # Plot the waveform\\n    plt.figure(figsize=(10, 5))\\n    pd.Series(y).plot(\\n                  lw=1,\\n                  title=title,\\n                 )\\n    plt.title(title)\\n    plt.xlabel(\"Time (seconds)\")\\n    plt.ylabel(\"Amplitude\")\\n    plt.grid(True)\\n    st.pyplot(plt)\\n\\n\\ndef plot_spect(y, title):\\n    # Plot the spectogram\\n    fig, ax = plt.subplots(figsize=(10, 5))\\n    img = librosa.display.specshow(y,\\n                                x_axis=\\'time\\',\\n                                y_axis=\\'log\\',\\n                                ax=ax)\\n    ax.set_title(title, fontsize=20)\\n    fig.colorbar(img, ax=ax, format=f\\'%0.2f\\')\\n    st.pyplot(plt)\\n\\ndef resize_spectrogram(original_shape, processed_spectrogram):\\n    # Extract the original shape\\n    original_rows, original_cols = original_shape', metadata={'source': 'repos/app.py'}), Document(page_content='def resize_spectrogram(original_shape, processed_spectrogram):\\n    # Extract the original shape\\n    original_rows, original_cols = original_shape\\n\\n    # Extract the processed shape\\n    processed_rows, processed_cols = processed_spectrogram.shape\\n\\n    # Calculate the row and column differences\\n    row_diff = original_rows - processed_rows\\n    col_diff = original_cols - processed_cols\\n\\n    # Pad or crop the processed spectrogram to match the original shape\\n    if row_diff > 0:\\n        # Pad along the rows\\n        pad_width = ((0, row_diff), (0, 0))\\n        processed_spectrogram = np.pad(processed_spectrogram, pad_width, mode=\\'constant\\', constant_values=0)\\n    elif row_diff < 0:\\n        # Crop along the rows\\n        processed_spectrogram = processed_spectrogram[:original_rows, :]\\n\\n    if col_diff > 0:\\n        # Pad along the columns\\n        pad_width = ((0, 0), (0, col_diff))\\n        processed_spectrogram = np.pad(processed_spectrogram, pad_width, mode=\\'constant\\', constant_values=0)\\n    elif col_diff < 0:\\n        # Crop along the columns\\n        processed_spectrogram = processed_spectrogram[:, :original_cols]\\n\\n    return processed_spectrogram\\ndef resize_spectrogram(original_shape, processed_spectrogram):\\n    # Extract the original shape\\n    original_rows, original_cols = original_shape\\n\\n    # Extract the processed shape\\n    processed_rows, processed_cols = processed_spectrogram.shape\\n\\n    # Calculate the row and column differences\\n    row_diff = original_rows - processed_rows\\n    col_diff = original_cols - processed_cols\\n\\n    # Pad or crop the processed spectrogram to match the original shape\\n    if row_diff > 0:\\n        # Pad along the rows\\n        pad_width = ((0, row_diff), (0, 0))\\n        processed_spectrogram = np.pad(processed_spectrogram, pad_width, mode=\\'constant\\', constant_values=0)\\n    elif row_diff < 0:\\n        # Crop along the rows\\n        processed_spectrogram = processed_spectrogram[:original_rows, :]\\n\\n    if col_diff > 0:\\n        # Pad along the columns\\n        pad_width = ((0, 0), (0, col_diff))\\n        processed_spectrogram = np.pad(processed_spectrogram, pad_width, mode=\\'constant\\', constant_values=0)\\n    elif col_diff < 0:\\n        # Crop along the columns\\n        processed_spectrogram = processed_spectrogram[:, :original_cols]\\n\\n    return processed_spectrogram\\n\\n# Function to denoise the audio (replace this with your denoising logic)\\ndef denoise_audio(input_audio):\\n    # Dummy denoising logic for demonstration\\n    return input_audio\\n\\ndef nearest_power_of_2(number):\\n    power = 1\\n    while power < number:\\n        power *= 2\\n    return power\\n\\n# List of predefined audio files\\naudio_files = {\\n    \"Noised Audio 1\": \"noisy speech/nosied_p237_106.wav\",\\n    \"Noised Audio 2\": \"noisy speech/nosied_p245_281.wav\",\\n    \"Noised Audio 3\": \"noisy speech/nosied_p246_051.wav\",\\n    \"Noised Audio 4\": \"noisy speech/nosied_p246_349.wav\",\\n    \"Noised Audio 5\": \"noisy speech/nosied_p237_271.wav\",\\n    \"Noised Audio 6\": \"noisy speech/nosied_p237_279.wav\",\\n    \"Noised Audio 7\": \"noisy speech/nosied_p241_292.wav\",\\n    \"Noised Audio 8\": \"noisy speech/nosied_p245_191.wav\",\\n    \"Noised Audio 9\": \"noisy speech/nosied_p245_289.wav\",\\n    \"Noised Audio 10\": \"noisy speech/nosied_p241_297.wav\",\\n    \"Noised Audio 11\": \"noisy speech/nosied_p241_312.wav\",\\n    \"Noised Audio 12\": \"noisy speech/nosied_p245_188.wav\",\\n    \"Noised Audio 13\": \"noisy speech/nosied_p260_108.wav\",\\n    \"Noised Audio 14\": \"noisy speech/nosied_p260_173.wav\",\\n    \"Noised Audio 15\": \"noisy speech/nosied_p260_357.wav\",\\n    \"Testing audio 1\": \"noisy speech/nosied_clnsp10.wav\",\\n    \"Testing audio 2\": \"noisy speech/nosied_clnsp22.wav\",\\n    \"Testing audio 3\": \"noisy speech/nosied_clnsp39.wav\",\\n    # Add more audio files as needed\\n}\\n\\n# Streamlit app\\ndef main():\\n    st.title(\"Speech Denoising with UNET\")', metadata={'source': 'repos/app.py'}), Document(page_content='# Streamlit app\\ndef main():\\n    st.title(\"Speech Denoising with UNET\")\\n\\n    st.subheader(\"Overview\")\\n    # Introduction\\n    st.write(\\n        \"Welcome to the Audio Denoising and Reconstruction App! \"\\n        \"This app showcases results of the Speech Denoiser Project. You can visualize the waveform, spectrogram of the original and recontrusted audio \"\\n        \"and apply a denoising process to reconstruct a cleaner version of the audio. \"\\n        \"The denoising process is done using UNET architecture\"\\n    )\\n\\n    st.subheader(\"Overview\")\\n    # Introduction\\n    st.write(\\n        \"Welcome to the Audio Denoising and Reconstruction App! \"\\n        \"This app showcases results of the Speech Denoiser Project. You can visualize the waveform, spectrogram of the original and recontrusted audio \"\\n        \"and apply a denoising process to reconstruct a cleaner version of the audio. \"\\n        \"The denoising process is done using UNET architecture\"\\n    )\\n\\n    # Select an audio file from the predefined list\\n    selected_file = st.selectbox(\"Select an Audio File\", list(audio_files.keys()))\\n\\n    # Display the selected audio file\\n    st.audio(audio_files[selected_file], format=\"audio/wav\")\\n\\n    # Convert the selected file to numpy array (replace this with actual data loading)\\n    audio, _ = librosa.load(audio_files[selected_file])\\n    spectrogram= extract(audio)\\n    original_shape = spectrogram.shape\\n\\n    # Display the original waveform if the checkbox is selected\\n    if st.checkbox(\"Show Waveform\", key= \"noised waveform1\"):\\n        st.header(\"Noised Speech Waveform\")\\n        plot_waveform(audio, title=\"Noised Speech Waveform\")\\n\\n    # Show the Spectrogram\\n    if st.checkbox(\"Show Spectrogram\", key=\\'noised spec1\\'):\\n        st.header(\"Noised Speech Spectrogram\")\\n        plot_spect(spectrogram, \"Noised Speech Spectrogram\")\\n\\n    original_shape = spectrogram.shape\\n\\n\\n\\n    # Normalizing\\n    xshape = 256\\n    y = spectrogram.shape[1]\\n    y = nearest_power_of_2(number=y)\\n    min_val = np.min(spectrogram)\\n    max_val = np.max(spectrogram)\\n    normalized_spec = normalize(spectrogram, min_val, max_val)\\n\\n    # Changing shape and loading as numpy array\\n    test_data = load_numpy(normalized_spec, (256, y))\\n\\n    # Creating model\\n    model = create_model((256, y))\\n\\n    # Button to trigger denoising and reconstruction\\n    if st.button(\"Reconstruct\"):\\n        # Denoise and reconstruct the audio\\n        denoised_audio = model.reconstruct(test_data)\\n\\n        # Display the denoised waveform\\n        st.header(\"Denoised and Reconstructed Audio\")\\n        signal, constructed_spectrogram = convert_spectrogram_to_audio(denoised_audio[0], min_val, max_val)\\n        signal, constructed_spectrogram = convert_spectrogram_to_audio(denoised_audio[0], min_val, max_val)\\n\\n        # reshaping to original shape\\n        constructed_spectrogram = resize_spectrogram(original_shape, constructed_spectrogram)\\n        # reshaping to original shape\\n        constructed_spectrogram = resize_spectrogram(original_shape, constructed_spectrogram)\\n        signal, _ = librosa.effects.trim(signal, top_db=20)\\n\\n\\n        sf.write(\\'constructed.wav\\', signal, 22050)\\n        st.audio(\\'constructed.wav\\', format=\"audio/wav\")\\n\\n        \\n        st.subheader(\"Reconstructed Audio Waveform\")\\n        plot_waveform(signal, \\'Reconstructed Audio Waveform\\')\\n\\n    \\n        st.subheader(\"Recontructed Audio Spectrogram\")\\n        plot_spect(constructed_spectrogram, \\'Contructed Audio after Noise removal\\')\\n\\n    \\n        \\n        st.subheader(\"Reconstructed Audio Waveform\")\\n        plot_waveform(signal, \\'Reconstructed Audio Waveform\\')\\n\\n    \\n        st.subheader(\"Recontructed Audio Spectrogram\")\\n        plot_spect(constructed_spectrogram, \\'Contructed Audio after Noise removal\\')\\n\\n    \\n\\n\\nif __name__ == \"__main__\":\\n    main()', metadata={'source': 'repos/app.py'}), Document(page_content='import keras\\nimport warnings\\nimport numpy as np\\n\\nclass IoUCheckpoint(keras.callbacks.Callback):\\n    def __init__(self, filepath, monitor=\\'iou\\', verbose=1, save_best_only=True, mode=\\'max\\'):\\n        super(IoUCheckpoint, self).__init__()\\n        self.filepath = filepath\\n        self.monitor = monitor\\n        self.verbose = verbose\\n        self.save_best_only = save_best_only\\n        self.mode = mode\\n        self.best_iou = -np.Inf\\n\\n    def on_epoch_end(self, epoch, logs=None):\\n        current_iou = logs.get(self.monitor)\\n        if current_iou is None:\\n            warnings.warn(f\\'Cannot save best model. {self.monitor} is not available.\\', RuntimeWarning)\\n        else:\\n            if current_iou > self.best_iou:\\n                if self.verbose > 0:\\n                    print(f\"\\\\nEpoch {epoch + 1}: {self.monitor} improved from {self.best_iou} to {current_iou}.\")\\n                self.best_iou = current_iou\\n                if self.save_best_only:\\n                    # file_path = self.filepath.format(epoch=epoch + 1, iou=current_iou)\\n                    self.model.save_weights(self.filepath, overwrite = True)\\n                    if self.verbose > 0:\\n                        print(f\"\\\\nSaved best model based on {self.monitor} to {self.filepath}\")\\n\\n    \\n\\ndef callbackList(filepath = \"/content/drive/My Drive/Colab Notebooks/speech denoiser/training/best_weight.hdf5\", verbose=1, save_best_only=True):\\n    iou_checkpoint = IoUCheckpoint(filepath=filepath, monitor=\\'iou\\', verbose=verbose, save_best_only=save_best_only)\\n\\n    # Adding the callback to the list of callbacks during model training\\n    callbacks = [iou_checkpoint]\\n    return callbacks', metadata={'source': 'repos/callback.py'}), Document(page_content='import os\\nimport pickle\\n\\nimport numpy as np\\nimport soundfile as sf\\nfrom speechgenerator import SoundGenerator\\n\\nfrom unet import UNET\\n\\n\\n# constants\\nHOP_LENGTH = 256\\nSAMPLE_RATE = 22050\\n\\n\\nSAVED_MODEL_DIR = \\'best_weight.hdf5\\'\\nMIN_MAX_VALUES_PATH = \\'min_max_value_save/min_max_values.pkl\\'\\nSPECTROGRAMS_PATH = \\'x_train_noised_speech\\'\\nSAVE_DIR_ORIGINAL = \"samples/original/\"\\nSAVE_DIR_GENERATED = \"samples/generated/\"\\n\\n\\ndef load_fsdd(spectrograms_path, final_shape):\\n    arr = []\\n    file_paths = []\\n    for root, _, file_names in os.walk(spectrograms_path):\\n        for file_name in file_names:\\n            file_path = os.path.join(root, file_name)\\n            # Pad the spectrogram to match the desired final shape\\n            # Load the .npy file and append it to the x_train list\\n            spectrogram = np.load(file_path)\\n            if spectrogram.shape[1] < final_shape[1]:\\n                # Pad the spectrogram to match the desired final shape\\n                pad_width = ((0, 0), (0, final_shape[1] - spectrogram.shape[1]))\\n                processed_spectrogram = np.pad(spectrogram, pad_width, mode=\\'constant\\', constant_values=0)\\n            elif spectrogram.shape[1] > final_shape[1]:\\n                # Trim the spectrogram to match the desired final shape\\n                processed_spectrogram = spectrogram[:final_shape[0], :final_shape[1]]\\n            else:\\n                processed_spectrogram = spectrogram  # No change needed if the shape is already as desired\\n            # Append the padded spectrogram to the x_train list\\n            processed_spectrogram = processed_spectrogram[:final_shape[0], :]\\n            arr.append(processed_spectrogram)\\n            file_paths.append(file_path)\\n    # Convert the list to a NumPy array if required\\n    arr = np.array(arr)\\n\\n    # reshaping to make it suitable for training\\n    arr = arr.reshape(arr.shape[0], arr.shape[1], arr.shape[2], 1 )\\n    # print(arr.shape)\\n    return arr, file_paths\\n\\n\\ndef select_spectrograms(spectrograms,\\n                        file_paths,\\n                        min_max_values,\\n                        num_spectrograms=2):\\n    sampled_indexes = np.random.choice(range(len(spectrograms)), num_spectrograms)\\n    sampled_spectrogrmas = spectrograms[sampled_indexes]\\n    file_paths = [file_paths[index] for index in sampled_indexes]\\n    sampled_min_max_values = [min_max_values[file_path] for file_path in\\n                           file_paths]\\n    print(file_paths)\\n    print(sampled_min_max_values)\\n    return sampled_spectrogrmas, sampled_min_max_values\\n\\n\\ndef save_signals(signals, save_dir, sample_rate=22050):\\n    for i, signal in enumerate(signals):\\n        save_path = os.path.join(save_dir, str(i) + \".wav\")\\n        sf.write(save_path, signal, sample_rate)\\n        print(\"saved at :\", save_path)\\n\\n\\nif __name__ == \"__main__\":\\n    # initialise sound generator\\n    model = UNET(\\n        input_shape=(256, 512, 1),\\n        conv_filters=(64, 128, 256, 512),\\n        conv_kernels=(3, 3),\\n    )\\n    model.load_weights(SAVED_MODEL_DIR)\\n\\n    sound_generator = SoundGenerator(model, HOP_LENGTH)\\n\\n    # load spectrograms + min max values\\n    with open(MIN_MAX_VALUES_PATH, \"rb\") as f:\\n        min_max_values = pickle.load(f)\\n\\n    specs, file_paths = load_fsdd(SPECTROGRAMS_PATH, (256,512))\\n\\n    # sample spectrograms + min max values\\n    sampled_specs, sampled_min_max_values = select_spectrograms(specs,\\n                                                                file_paths,\\n                                                                min_max_values,\\n                                                                5)\\n    \\n\\n    # generate audio for sampled spectrograms\\n    signals = sound_generator.generate(sampled_specs,\\n                                          sampled_min_max_values)\\n\\n    # convert spectrogram samples to audio\\n    original_signals = sound_generator.convert_spectrograms_to_audio(\\n        sampled_specs, sampled_min_max_values)', metadata={'source': 'repos/generate.py'}), Document(page_content='# convert spectrogram samples to audio\\n    original_signals = sound_generator.convert_spectrograms_to_audio(\\n        sampled_specs, sampled_min_max_values)\\n\\n    # save audio signals\\n    save_signals(signals, SAVE_DIR_GENERATED)\\n    save_signals(original_signals, SAVE_DIR_ORIGINAL)', metadata={'source': 'repos/generate.py'}), Document(page_content='\"\"\"\\n1. load clean and noise audio files\\n\\nfor each file in clean audio file perform the following steps:\\n2. trim clean and noise audio files\\n3. mix clean audio with random noise\\n4. generate log spectogram of noisy speech\\n5. mix max normalizarion of noisy speech\\n6. save log spectogram in X_train_spec folder\\n7. store orginal min max value of each log spectogram as dictionary {\\'save_path\\':{\\'min\\': ,\\'max\\': }}\\n\\nfinally save the pickle file of min max value\\n\"\"\"\\n\\nimport numpy as np\\nimport os\\nimport random\\nimport pickle\\nimport librosa\\n\\n\\nclass Loader:\\n    \"\"\"Loader is responsible for loading an audio file.\"\"\"\\n    def __init__(self, sample_rate, mono):\\n        self.sample_rate = sample_rate\\n        self.mono = mono\\n\\n    def load(self, file_path):\\n        signal = librosa.load(file_path,\\n                              sr=self.sample_rate,\\n                              mono=self.mono)[0]\\n        return signal\\n    \\n\\n    \\nclass Trimmer:\\n    \"\"\"Trimmer is responsible for triming the silence in audio\"\"\"\\n    def __init__(self, top_db = 20, hop_length = 256) -> None:\\n        self.top_db = top_db\\n        self.hop_length = hop_length\\n\\n    def trim_audio(self, audio):\\n        audio_trimmed, _ = librosa.effects.trim(audio, top_db=self.top_db, hop_length=self.hop_length)\\n        return audio_trimmed\\n    \\n\\n\\nclass Mixer:\\n    \"\"\"Mixer is responsible for mixing two audios\"\"\"\\n    def __init__(self, overlap_ratio) -> None:\\n        self.overlap_ratio = overlap_ratio\\n\\n    def mix_audio(self, clean_audio, noise_audio):\\n        mixed_audio = clean_audio + noise_audio[:len(clean_audio)] * self.overlap_ratio\\n        return mixed_audio\\n    \\n\\n\\nclass LogSpectrogramExtractor:\\n    \"\"\"Responsible for extraing the log spectrogram (in DB) of the \\n    time series audio\"\"\"\\n    def __init__(self, frame_size, hop_length) -> None:\\n        self.frame_size = frame_size\\n        self.hop_length = hop_length\\n\\n    def extract(self, signal):\\n        stft = librosa.stft(signal,\\n                                n_fft=self.frame_size,\\n                                hop_length=self.hop_length)\\n        spectrogram = np.abs(stft)\\n        log_spectrogram = librosa.amplitude_to_db(spectrogram)\\n        return log_spectrogram\\n    \\n\\nclass MinMaxNormalizer:\\n    \"\"\"MinMaxNormaliser applies min max normalisation to an spectrogram(array)\"\"\"\\n    def __init__(self) -> None:\\n        pass\\n\\n    def normalize(self, spectrogram, min_val, max_val):\\n        normalized_spectrogram = (spectrogram - min_val) / (max_val - min_val)\\n        return normalized_spectrogram\\n\\n    def denormalize(self, spectrogram, original_min, original_max):\\n        denormalized_spectrogram = spectrogram * (original_max - original_min) + original_min\\n        return denormalized_spectrogram\\n\\n\\nclass Saver:\\n    \"\"\"saver is responsible to save features, min max values, and pickle file.\"\"\"\\n    def __init__(self, clean_feature_folder, noise_feature_folder) -> None:\\n        self.clean_feature_foler = clean_feature_folder\\n        self.noise_feature_foler = noise_feature_folder\\n\\n    def save_normalized_spectrogram(self, spectrogram, filename, clean):\\n        if clean:\\n            save_path = os.path.join(self.clean_feature_foler, f\"{filename}.npy\")\\n            np.save(save_path, spectrogram)\\n            return save_path\\n        else:\\n            save_path = os.path.join(self.noise_feature_foler, f\"{filename}.npy\")\\n            np.save(save_path, spectrogram)\\n            return save_path\\n        \\n    \\n    def save_min_max_value(self, dir, data):\\n        save_path = os.path.join(dir, \"min_max_values.pkl\")\\n        with open(save_path, \\'wb\\') as f:\\n            pickle.dump(data, f)\\n\\n\\nclass PreprocessingPipeline:\\n    \"\"\"PreprocessingPipeline processes audio files in a directory, applying\\n    the following steps to each file:\\n        1- load a file\\n        2- trim the signal\\n        3- mix signal with noise for training\\n        3- extracting log spectrogram from signal\\n        4- normalise spectrogram\\n        5- save the normalised spectrogram', metadata={'source': 'repos/preprocess.py'}), Document(page_content='Storing the min max values for all the log spectrograms.\\n    \"\"\"\\n    def __init__(self):\\n        self.loader = None\\n        self.trimmer = None\\n        self.mixer = None\\n        self.extractor = None\\n        self.normaliser = None\\n        self.saver = None\\n        self.min_max_values = {}\\n\\n    def process(self, clean_audio_dir, noise_audio_dir, min_max_value_dir):\\n        for filename in os.listdir(clean_audio_dir):\\n            if filename.endswith(\".wav\"):\\n                clean_audio_path = os.path.join(clean_audio_dir, filename)\\n                noise_audio_path = random.choice(os.listdir(noise_audio_dir))\\n                noise_audio_path = os.path.join(noise_audio_dir, noise_audio_path)\\n                self._process_file(filename, clean_audio_path, noise_audio_path)\\n\\n        self.saver.save_min_max_value(min_max_value_dir,self.min_max_values)\\n\\n    \\n    def _process_file(self,filename,  clean_audio_path, noise_audio_path):\\n        clean_audio = self.loader.load(clean_audio_path)\\n        trimmed_clean_audio = self.trimmer.trim_audio(clean_audio)\\n\\n        noise_audio = self.loader.load(noise_audio_path)\\n        trimmed_noise_audio = self.trimmer.trim_audio(noise_audio)\\n\\n        mixed_audio = self.mixer.mix_audio(trimmed_clean_audio, trimmed_noise_audio)\\n\\n        log_spec_noisy = self.extractor.extract(mixed_audio)\\n        log_spec_clean = self.extractor.extract(trimmed_clean_audio)\\n\\n        normalized_spec_noisy = self.normaliser.normalize(log_spec_noisy, np.min(log_spec_noisy), np.max(log_spec_noisy))\\n        normalized_spec_clean = self.normaliser.normalize(log_spec_clean, np.min(log_spec_clean), np.max(log_spec_clean))\\n\\n        save_path_noisy = self.saver.save_normalized_spectrogram(normalized_spec_noisy, f\\'{filename}_spec\\', False)\\n        save_path_clean = self.saver.save_normalized_spectrogram(normalized_spec_clean, f\\'{filename}_spec\\', True)\\n\\n        self._store_min_max_value(save_path_noisy, log_spec_noisy.min(), log_spec_noisy.max())\\n        print(f\"Processed Noisy file: {save_path_noisy}, Processed Clean file: {save_path_clean}\")\\n\\n    def _store_min_max_value(self, save_path, min_val, max_val):\\n        self.min_max_values[save_path] = {\\n            \"min\": min_val,\\n            \"max\": max_val\\n        }\\n\\n\\n\\n    \\nif __name__ == \"__main__\":\\n    SAMPLE_RATE = 22050\\n    MONO = True\\n    HOP_LENGTH = 256\\n    TOP_DB = 20\\n    FRAME_SIZE = 512\\n    OVERLAP_RATIO=0.3\\n\\n    # Defining the paths to the clean audio and noise audio folders\\n    clean_audio_dir = \\'D:/Speech data/MS-SNSD/clean_train\\'\\n    noise_audio_dir = \\'D:/Speech data/MS-SNSD/noise_train\\'\\n\\n    # folder for storing spectogram value of noised and clean audio speech\\n    X_train_spec_dir = \\'x_train_noised_speech\\'\\n    Y_train_spec_dir = \\'y_train_clean_audio\\'\\n\\n    # folder for storing min max values dictionary\\n    min_max_value_dir = \\'min_max_value_save\\'\\n\\n    # Ensure the output folders exist\\n    if not os.path.exists(X_train_spec_dir):\\n        os.makedirs(X_train_spec_dir)\\n    if not os.path.exists(Y_train_spec_dir):\\n        os.makedirs(Y_train_spec_dir)\\n    if not os.path.exists(min_max_value_dir):\\n        os.makedirs(min_max_value_dir)\\n\\n    # instantiate all objects\\n    loader = Loader(sample_rate=SAMPLE_RATE, \\n                    mono=MONO)\\n    trimmer = Trimmer(hop_length=HOP_LENGTH, \\n                      top_db=TOP_DB)\\n    mixer = Mixer(overlap_ratio=OVERLAP_RATIO)\\n    log_spectrogram_extractor = LogSpectrogramExtractor(frame_size=FRAME_SIZE, \\n                                                        hop_length=HOP_LENGTH)\\n    min_max_normalizer = MinMaxNormalizer()\\n    saver = Saver(clean_feature_folder=Y_train_spec_dir,\\n                  noise_feature_folder=X_train_spec_dir,)', metadata={'source': 'repos/preprocess.py'}), Document(page_content='preprocessing_pipeline = PreprocessingPipeline()\\n    preprocessing_pipeline.loader = loader\\n    preprocessing_pipeline.trimmer = trimmer\\n    preprocessing_pipeline.mixer = mixer\\n    preprocessing_pipeline.extractor = log_spectrogram_extractor\\n    preprocessing_pipeline.normaliser = min_max_normalizer\\n    preprocessing_pipeline.saver = saver\\n\\n    preprocessing_pipeline.process(clean_audio_dir=clean_audio_dir,\\n                                   noise_audio_dir=noise_audio_dir,\\n                                   min_max_value_dir=min_max_value_dir)', metadata={'source': 'repos/preprocess.py'}), Document(page_content=\"# Speech Denoiser with UNets\\n\\n## Overview\\n\\nThis repository contains code for a Speech Denoising model based on the UNet architecture. The model is trained on a dataset of 1700 samples, comprising clean and noised audio recordings. The UNet architecture is widely used for image segmentation tasks and has been adapted here for the denoising of audio signals.\\n\\n## Introduction\\n\\nDenoising of speech signals is a critical task in various audio processing applications. This repository provides a solution based on the UNet architecture, a convolutional neural network (CNN) commonly used for image segmentation but adapted here for denoising audio.\\n\\n## Dataset\\n\\nThe dataset consists of 1700 samples, each comprising a clean audio recording and its corresponding noised version. The data was carefully selected to cover a variety of scenarios, ensuring the model's robustness in denoising different types of audio signals.\\n\\n## Methodology\\n\\n### Data Preprocessing\\n\\nThe preprocessing steps include loading clean and noise audio files, trimming unnecessary segments, mixing clean audio with random noise audio, and generating log spectrograms of the noisy speech as well as clean audio speech. The noisy speech spectrogram is used as input and clean audio speech is used as desired output for the model. Additionally, the original min-max values of each log spectrogram are saved for later use during denoising.\\n\\n### Spectrogram Generation\\nSpectrograms play a crucial role in this denoising project, serving as a key input to the UNet model. A spectrogram is a visual representation of the spectrum of frequencies in a sound signal as they vary with time. In the context of this project, we employ log spectrograms, which offer enhanced sensitivity to audio features.\\nThe process begins by applying the Short-Time Fourier Transform (STFT) to the mixed audio signal. The STFT breaks the audio signal into smaller segments, providing a time-varying representation of the signal's frequency content. The amplitude of each frequency bin is then converted to decibels using the librosa.amplitude_to_db function, resulting in a log spectrogram.\\n\\n### UNet Architecture\\n\\nThe UNet architecture used in this project comprises downsampling and upsampling blocks. The downsampling block extracts features from the input audio spectrograms, while the upsampling block reconstructs the denoised audio signal spectrograms. Batch normalization and dropout layers are incorporated to enhance model generalization. Skip connections are formed by concatenating the upsampled layer with the corresponding downsampling block's output. This structure enables the model to retain fine-grained details during reconstruction. The final layer of the UNet architecture contains a single filter with a sigmoid activation function. This configuration is suitable for binary classification tasks, making it fitting for the denoising objective. The model outputs denoised log spectrograms.\\n\\n### Training\\n\\nDue to memory constraints, the model is trained on a subset of 1700 samples, resulting in potential overfitting. Training on a larger dataset (e.g., 20000 samples) is limited by GPU memory requirements. The model is trained using binary cross-entropy loss, optimizing for the denoised spectrogram output. The intersection over union (IoU) metric is employed to evaluate the model's performance during training.\\n\\n### Audio Signal Reconstruction\\n\\n#### Denormalization\\n\\nDuring post-processing, the stored min-max values are used to denormalize the denoised log spectrogram.\\n\\n#### Log Spectrogram to Audio Signal\\n\\nInverse log transformation is applied to obtain the amplitude spectrogram. The inverse STFT is then performed to convert the spectrogram back to a time-domain audio signal.\\n\\n## Results\\nThe audio results is included in the samples folder.\\n\\n### Waveforms\\n\\nBelow are visualizations of clean audio, noised audio, mixed audio, and noised audio waveforms:\\n\\n![Clean Audio Waveform](images/clean_audio_waveform.png)\", metadata={'source': 'repos/README.md'}), Document(page_content='### Waveforms\\n\\nBelow are visualizations of clean audio, noised audio, mixed audio, and noised audio waveforms:\\n\\n![Clean Audio Waveform](images/clean_audio_waveform.png)\\n\\n![Mixed Audio Waveform](images/mixed_audio_waveform.png)\\n\\n![Denoised Audio Waveform](images/contructed_audio_waveform.png)\\n\\n### Spectrograms\\n\\nSample spectrograms:\\n\\n![Clean Audio Spectrogram](images/clean_audio_spectrogram.png)\\n\\n![Noise Spectrogram](images/noise_audio_spectrogram.png)\\n\\n![Mixed Audio Spectrogram](images/mixed_audio_spectrogram.png)\\n\\n![Denoised Audio Spectrogram](images/contructed_audio_from_model.png)\\n\\n## Limitations\\n\\nThe model has certain limitations:\\n\\n1. **Overfitting**: The model may exhibit overfitting due to the limited training data (1700 samples).\\n2. **Memory Constraints**: Training on a larger dataset (e.g., 20000 samples) is restricted by GPU memory requirements.', metadata={'source': 'repos/README.md'}), Document(page_content='import librosa\\nfrom preprocess import MinMaxNormalizer\\n\\n\\nclass SoundGenerator:\\n    \"\"\"SoundGenerator is responsible for generating audios from\\n    spectrograms.\\n    \"\"\"\\n    def __init__(self, model, hop_length):\\n        self.model = model\\n        self.hop_length = hop_length\\n        self._min_max_normaliser = MinMaxNormalizer()\\n\\n    def generate(self, spectrograms, min_max_values):\\n        generated_spectrograms=self.model.reconstruct(spectrograms)\\n        signals = self.convert_spectrograms_to_audio(generated_spectrograms, min_max_values)\\n        return signals\\n    \\n    def convert_spectrograms_to_audio(self, spectrograms, min_max_values):\\n        signals = []\\n        # reshape the log spectrogram\\n        spectrograms = spectrograms.reshape(spectrograms.shape[0], spectrograms.shape[1], spectrograms.shape[2])\\n\\n        for spectrogram, min_max_value in zip(spectrograms, min_max_values):\\n            \\n            # apply denormalisation\\n            denorm_log_spec = self._min_max_normaliser.denormalize(\\n                spectrogram, min_max_value[\"min\"], min_max_value[\"max\"])\\n            # log spectrogram -> spectrogram\\n            spec = librosa.db_to_amplitude(denorm_log_spec)\\n            # apply Griffin-Lim\\n            signal = librosa.istft(spec, hop_length=self.hop_length)\\n            # append signal to \"signals\"\\n            signals.append(signal)\\n        return signals', metadata={'source': 'repos/speechgenerator.py'}), Document(page_content='import os\\nimport numpy as np\\nfrom unet import UNET\\nfrom callback import callbackList\\n\\nx_train_dir = \\'/content/drive/MyDrive/Colab Notebooks/speech denoiser/x_train_noised_speech\\'\\ny_train_dir = \\'/content/drive/MyDrive/Colab Notebooks/speech denoiser/y_train_clean_audio\\'\\n\\nSHAPE=(256, 128)\\nLEARNING_RATE = 0.001\\nBATCH_SIZE = 10\\nEPOCHS = 10\\n\\n\\n# getting the maximum shape of the numpy array\\ndef get_max_shape(dir):\\n    max_y = 0\\n    max_x = 0\\n    for filename in os.listdir(dir):\\n        if filename.endswith(\".npy\"):\\n            file_path = os.path.join(dir, filename)\\n\\n            # Load the .npy file and append it to the x_train list\\n            spectrogram = np.load(file_path)\\n            if spectrogram.shape[1] > max_y:\\n                max_y = spectrogram.shape[1]\\n\\n            if spectrogram.shape[0] > max_x:\\n                max_x = spectrogram.shape[0]\\n\\n    return (max_x, max_y)\\n\\n# load numpy spectrograms\\ndef load_array(dir, final_shape):\\n    # Initialize an empty list to store the loaded spectrograms\\n    arr = []\\n    # Get the list of files in the directory and sort them alphabetically\\n    file_list = sorted(os.listdir(dir))\\n    # Iterate through each file in the directory\\n    for filename in file_list:\\n        if filename.endswith(\".npy\"):\\n            file_path = os.path.join(dir, filename)\\n            print(file_path)\\n\\n            # Load the .npy file and append it to the x_train list\\n            spectrogram = np.load(file_path)\\n            # Pad the spectrogram to match the desired final shape\\n            if spectrogram.shape[1] < final_shape[1]:\\n                # Pad the spectrogram to match the desired final shape\\n                pad_width = ((0, 0), (0, final_shape[1] - spectrogram.shape[1]))\\n                processed_spectrogram = np.pad(spectrogram, pad_width, mode=\\'constant\\', constant_values=0)\\n            elif spectrogram.shape[1] > final_shape[1]:\\n                # Trim the spectrogram to match the desired final shape\\n                processed_spectrogram = spectrogram[:final_shape[0], :final_shape[1]]\\n            else:\\n                processed_spectrogram = spectrogram  # No change needed if the shape is already as desired\\n            # Append the padded spectrogram to the x_train list\\n            processed_spectrogram = processed_spectrogram[:final_shape[0], :]\\n            arr.append(processed_spectrogram)\\n\\n    # Convert the list to a NumPy array if required\\n    arr = np.array(arr)\\n\\n    # reshaping to make it suitable for training\\n    arr = arr.reshape(arr.shape[0], arr.shape[1], arr.shape[2], 1 )\\n\\n    return arr\\n\\n\\ndef train(x_train, y_train, learning_rate, batch_size, epochs, callbacks):\\n    model = UNET(\\n        input_shape=(256, 512, 1),\\n        conv_filters=(64, 128, 256, 512),\\n        conv_kernels=(3, 3),\\n    )\\n    model.summary()\\n    model.compile(learning_rate)\\n    history=model.train(x_train, y_train, batch_size, epochs, callbacks)\\n    return model, history\\n\\n\\nif __name__ == \"__main__\":\\n    shape=get_max_shape(x_train_dir)\\n    print(shape)\\n    x_train = load_array(x_train_dir, SHAPE)\\n    y_train = load_array(y_train_dir, SHAPE)\\n    callbacks = callbackList()\\n    model, history = train(x_train, \\n                           y_train, \\n                           LEARNING_RATE, \\n                           BATCH_SIZE, \\n                           EPOCHS, \\n                           callbacks)\\n    model.save(\"/content/drive/MyDrive/Colab Notebooks/speech denoiser\")', metadata={'source': 'repos/train.py'}), Document(page_content='import tensorflow as tf\\nfrom tensorflow.keras import layers, models\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras import backend as k\\nimport matplotlib.pyplot as plt\\nimport os\\nimport pickle\\n\\nclass UNET:\\n    \"\"\"\\n    Unet represents a Deep Convolutional encoder decoder architecture\\n    with skip connection.\\n    \"\"\"\\n    def __init__(self,\\n                 input_shape,\\n                 conv_filters,\\n                 conv_kernels,):\\n        self.input_shape = input_shape # [256, 256, 1]\\n        self.conv_filters = conv_filters # [64, 1280, 256, 512]\\n        self.conv_kernels = conv_kernels # (3, 3)    \\n\\n\\n        self.model = None\\n\\n        self._model_input = None\\n\\n        self._build()\\n\\n    def summary(self):\\n        self.model.summary()\\n\\n    def compile(self, learning_rate=0.001):\\n        optimizer = Adam(learning_rate=learning_rate)\\n        binary_loss = tf.keras.losses.BinaryCrossentropy()\\n        self.model.compile(optimizer=optimizer,\\n                           loss=binary_loss,\\n                           metrics=[self.iou,])\\n        \\n    def train_generator(self, train_generator, valid_generator, batch_size, num_epochs, callbacks):\\n        history=self.model.fit(\\n                      train_generator,\\n                      batch_size = batch_size,\\n                      epochs=num_epochs,\\n                      callbacks=callbacks,\\n                      validation_data=valid_generator,\\n                       )\\n        return history\\n    \\n    def train(self, x_train, y_train, batch_size, num_epochs, callbacks):\\n        history=self.model.fit(\\n                      x_train,\\n                      y_train,\\n                      batch_size = batch_size,\\n                      epochs=num_epochs,\\n                      callbacks=callbacks,\\n                      validation_split=0.2,\\n                       )\\n        return history\\n    \\n        \\n    def save(self, save_folder):\\n        self._create_folder_if_it_doesnt_exist(save_folder)\\n        self._save_parameters(save_folder)\\n        self._save_weights(save_folder)\\n\\n    def load_weights(self, weights_path):\\n        self.model.load_weights(weights_path)\\n\\n    # For predicitons\\n    def reconstruct(self, spectrograms):\\n        predictions = self.model.predict(spectrograms)\\n        return predictions\\n    \\n    # Plots the graph for training and validation loss and iou score\\n    def plot_loss(self, history):\\n        plt.plot(history.history[\\'loss\\'],label=\"loss\")\\n        plt.plot(history.history[\\'val_loss\\'],label=\"validation loss\")\\n        plt.legend()\\n        plt.show()\\n\\n    def plot_iou(self, history):\\n        plt.plot(history.history[\\'iou\\'],label=\"iou\")\\n        plt.plot(history.history[\\'val_iou\\'],label=\"validation iou\")\\n        plt.legend()\\n        plt.show()\\n\\n    # Calculates the Iou score matrics    \\n    def iou(self, y_true, y_pred, smooth = 1):\\n        y_true = k.flatten(y_true)\\n        y_pred = k.flatten(y_pred)\\n        intersection = k.sum(y_true*y_pred)\\n        union = k.sum(y_true)+k.sum(y_pred)-intersection\\n        iou_score = (intersection+smooth)/(union+smooth)\\n        return iou_score\\n    \\n    def _create_folder_if_it_doesnt_exist(self, folder):\\n        if not os.path.exists(folder):\\n            os.makedirs(folder)\\n\\n    def _save_parameters(self, save_folder):\\n        parameters = [\\n            self.input_shape,\\n            self.conv_filters,\\n            self.conv_kernels,\\n        ]\\n        save_path = os.path.join(save_folder, \"parameters.pkl\")\\n        with open(save_path, \"wb\") as f:\\n            pickle.dump(parameters, f)\\n\\n    def _save_weights(self, save_folder):\\n        save_path = os.path.join(save_folder, \"weights.h5\")\\n        self.model.save_weights(save_path)\\n\\n    def _build(self):\\n        self._build_unet()\\n\\n    # Define the U-Net architecture\\n    def _build_unet(self):\\n        inputs = tf.keras.Input(self.input_shape)', metadata={'source': 'repos/unet.py'}), Document(page_content='def _build(self):\\n        self._build_unet()\\n\\n    # Define the U-Net architecture\\n    def _build_unet(self):\\n        inputs = tf.keras.Input(self.input_shape)\\n\\n        conv1, pool1 = self._downsample_block(inputs, 64, (3, 3))\\n        conv2, pool2 = self._downsample_block(pool1, 128, (3, 3))\\n        conv3, pool3 = self._downsample_block(pool2, 256, (3, 3))\\n        conv4, pool4 = self._downsample_block(pool3, 512, (3, 3))\\n\\n        conv5 = layers.Conv2D(1024, (3, 3), activation=\\'relu\\', padding=\\'same\\')(pool4)\\n        conv5 = layers.Conv2D(1024, (3, 3), activation=\\'relu\\', padding=\\'same\\')(conv5)\\n\\n        conv6 = self._upsample_block(conv5, conv4, 512, (3, 3))\\n        conv7 = self._upsample_block(conv6, conv3, 256, (3, 3))\\n        conv8 = self._upsample_block(conv7, conv2, 128, (3, 3))\\n        conv9 = self._upsample_block(conv8, conv1, 64, (3, 3))\\n\\n        outputs = layers.Conv2D(1, 1, activation=\\'sigmoid\\')(conv9)\\n\\n        self.model = models.Model(inputs=inputs, outputs=outputs, name = \\'Unet\\')\\n    \\n    # Downsampling block (encoder)\\n    def _downsample_block(self, input_layer, filters, kernel_size, padding=\\'same\\', activation=\\'relu\\'):\\n        conv1 = layers.Conv2D(filters, kernel_size, activation=activation, padding=padding)(input_layer)\\n        conv1 = layers.Dropout(0.1)(conv1)\\n        conv1 = layers.Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv1)\\n        b1 = layers.BatchNormalization()(conv1)\\n        r1 = layers.ReLU()(b1)\\n        pool = layers.MaxPooling2D(pool_size=(2, 2))(r1)\\n        # print(\"Downsample block shape is: \" ,conv1.shape)\\n        return conv1, pool\\n\\n    # Upsampling block\\n    def _upsample_block(self, input_layer, skip_connection, filters, kernel_size, padding=\\'same\\', activation=\\'relu\\'):\\n        up = layers.UpSampling2D(size=(2, 2))(input_layer)\\n        up = layers.Conv2DTranspose(filters, kernel_size, activation=activation, padding=padding)(up)\\n        merge = layers.concatenate([up, skip_connection], axis=3)\\n        conv = layers.Conv2D(filters, 3, activation=activation, padding=padding)(merge)\\n        conv = layers.Conv2D(filters, 3, activation=activation, padding=padding)(conv)\\n        # print(\"upsample block shape is: \" ,conv.shape)\\n        return conv\\n    \\n\\nif __name__ == \"__main__\":\\n    unet_model = UNET(\\n        input_shape=(256, 256, 1),\\n        conv_filters=(64, 128, 256, 512),\\n        conv_kernels=(3, 3),\\n    )\\n    unet_model.summary()', metadata={'source': 'repos/unet.py'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1027, which is longer than the specified 1000\n",
      "Created a chunk of size 1137, which is longer than the specified 1000\n",
      "Created a chunk of size 1366, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1297, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def split_docs(docs):\n",
    "    \"\"\"Split the documents into chunks\"\"\"\n",
    "    splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap =0)\n",
    "    texts = splitter.split_documents(docs)\n",
    "    return texts\n",
    "\n",
    "\n",
    "texts = split_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://samman/langchain_course_chat_with_gh already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 58 embeddings in 1 batches of size 58:: 100%|██████████| 1/1 [01:27<00:00, 87.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://samman/langchain_course_chat_with_gh', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (58, 768)  float32   None   \n",
      "    id        text      (58, 1)     str     None   \n",
      " metadata     json      (58, 1)     str     None   \n",
      "   text       text      (58, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "\n",
    "def create_database_and_store_texts(dataset_path, texts):\n",
    "    \"\"\"Create an empty deeplake database in specified path\"\"\"\n",
    "    embeddings = GooglePalmEmbeddings()\n",
    "    db = DeepLake(dataset_path, embedding = embeddings)\n",
    "    db.add_documents(texts)\n",
    "\n",
    "    return db\n",
    "\n",
    "my_activeloop_org_id = \"samman\"\n",
    "my_activeloop_dataset_name = \"langchain_course_chat_with_gh\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = create_database_and_store_texts(dataset_path, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def search_db(db, query):\n",
    "    \"\"\"Search for a response to the query in the DeepLake database.\"\"\"\n",
    "    # Create a retriever from the DeepLake instance\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    # Set the search parameters for the retriever\n",
    "    retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "    retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "    retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "    retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "    model = GoogleGenerativeAI(model='gemini-pro',  \n",
    "                               temperature=0,\n",
    "                               )\n",
    "    \n",
    "    # Create a RetrievalQA instance from the model and retriever\n",
    "    qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "    # Return the result of the query\n",
    "    return qa.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model used in this project is a UNet architecture, which is a convolutional neural network (CNN) commonly used for image segmentation. It has been adapted here for denoising audio signals. The UNet architecture consists of a downsampling path and an upsampling path. The downsampling path extracts features from the input audio spectrograms, while the upsampling path reconstructs the denoised audio signal spectrograms. Skip connections are formed by concatenating the upsampled layer with the corresponding downsampling block's output. This structure enables the model to retain fine-grained details during reconstruction. The final layer of the UNet architecture contains a single filter with a sigmoid activation function, making it suitable for binary classification tasks, which is appropriate for the denoising objective. The model outputs denoised log spectrograms.\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain architecture of model used in this project\"\n",
    "response = search_db(db, query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
